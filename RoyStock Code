##### get all needed packages #####
rm(list=ls())
pkg<-c("rvest","tidyquant","lubridate", "ggplot2", "reshape2", "dplyr")
lapply(pkg, require, character.only = TRUE)

#####

#here is the top parathesis of user defined function
RoyStocks<-function(n){

#first we need to know the stock codes

##### single web scraping: to get PS, and them reshape it in to data frame #####
#Reading the HTML code from the website (need html or selector gadget)
# .pnum , .nnum , .num , .linkb
# above are coordinate used for extracting right infos
#first we scrap the 100 popular stocks
library('rvest')
webpage <- read_html('http://www.wsj.com/mdc/public/page/2_3021-activnyse-actives.html')
#webpage
#length(webpage)

Top100Stocks <- html_text(html_nodes(webpage,'.pnum , .nnum , .num , .linkb'))
#head(Top100Stocks)

#Top100Stocks[7] 
# the "2" which should be on the second line is still in the 7th position of the first line
# we need to extract the data into 6 seperate vector and then combine it

# addtionally, we need to seperate Name and Code, using strsplit, end up with 7 columns in total
# single sample of extracting Code
#Name1<-Top100Stocks[8]
#Name1
#RawCode<-strsplit(Name1, " \\(")
#RawCode[[1]][2]
#RawCode[[1]][1] #this can be used as names
#NeatCode<-strsplit(RawCode[[1]][2], ")")
#NeatCode[[1]][1]

#now using loop to combine it into vectors of Name and Code
Code<-rep(1,100)
Name<-rep(1,100)
Serial<-rep(1,100)
Volume<-rep(1,100)
Price<-rep(1,100)
Chg<-rep(1,100)
Pchg<-rep(1,100)
for (i in 1:100){
  Name[i]<-Top100Stocks[i*6-4]
  RawCode<-strsplit(Name[i], "\\(")
  #RawCode[[1]][2]
  Name[i]<-RawCode[[1]][1]
  NeatCode<-strsplit(RawCode[[1]][2], ")")
  Code[i]<-NeatCode[[1]][1]
  # them theres the rest of the variables
  Serial[i]<-Top100Stocks[i*6-5]
  Volume[i]<-Top100Stocks[i*6-3]
  Price[i]<-Top100Stocks[i*6-2]
  Chg[i]<-Top100Stocks[i*6-1]
  Pchg[i]<-Top100Stocks[i*6]
}
#print(Code)
#print(Name)

#convert string numbers into numeric
#Volume<-suppressWarnings(as.numeric(Volume)) # the Volume seems uneasy to convert into numeric, right now it's still string
#Volume
#Price<-suppressWarnings(as.numeric(Price))
#Price
#Chg<-suppressWarnings(as.numeric(Chg))
#Chg
#Pchg<-suppressWarnings(as.numeric(Pchg))
#Pchg

PS<-cbind(Serial, Name, Code, Volume, Price, Chg, Pchg)
#dim(PS)
#head(PS)
PS<-as.data.frame(PS,stringsAsFactors=FALSE) 
# this tringsAsFactors =FALSE needed to ensure we can convert character to numeric
#colnames(PS)<-c('Serial', 'Name', 'Code', 'Volume', 'Price', 'Chg', 'Pchg')
#"PBRA" %in% PS$Code 
#this step is to find if the problematic stock 'PBRA' in the list
#PS<-PS[PS$Code!="PBRA",]
#dim(PS)
#str(PS)
#####

##### find out today's top n best performing stocks #####
#### (it is no meaning of capture all 100 stocks right?), 
#### and choose them to show their history prices for past 1 year
#summary(PS)
PS$Pchg<-as.numeric(PS$Pchg)
#PS$Pchg
#str(PS)
#?order
PSsort<-PS[order(as.numeric(PS$Pchg), decreasing=TRUE),]
head(PSsort)
#here I use percentage change "Pchg" as performance index, this can be set as adjustable parameter too if you like
#n<-3 if wanna test certain sections inside the function, need to manully specify the value of n here
TPS<-PSsort[1:n,3]
#TPS stands for top popular stocks

#### at this point we have 2 data files: 1) a list containing history price for 100 stocks, 
#### and 2) a list of stock codes sorted by their performance for today.

#####

#now we know the stock codes, we can start pulling off history data

##### sample code of how we get the history stock prices for the past year, not necessarily needed in the final function #####
#library(tidyquant)
#library(lubridate)
#from <- today() - years(1)
#Stock1 <- tq_get(PS[1,3], get = "stock.prices", from = from)
#Stock1

##### loop to get a list of n stock's historical prices #####
library(tidyquant)
library(lubridate)
Stock<-list()
for (i in 1:n){
  from <- today() - years(1)
  Stock[[toString(TPS[i])]]<- tq_get(toString(TPS[i]), get = "stock.prices", from = from)
}
#Stock
#length(Stock)

#####

##### url manipulation (May not useful but good to know)#####
#Specifying the url for desired website to be scrapped
#urlx <- 'https://finance.yahoo.com/quote/xxxx/profile?p=xxxx'

#url manuplation example
#url1<-gsub("xxxx", "BAC", urlx)
#url1

#url manuplation loop/list
#url<-list()

#Scodes<-PS[,3]
#Scodes
#Scode1<-PS[1,3]

#for (i in 1:100){
#  url[[toString(PS[i,2])]]<-gsub("xxxx",toString(PS[i,3]),urlx)
#}
#url
##### 

##### visualization part #####
# take a look at the data structure first
#TPS[1]
#Stock[["AA"]]$close # we use the close price to draw curves
#Stock[["AA"]]$date
#Stock[[toString(TPS[1])]]

#aggregate closing price for all n stocks into 1 dataframe
#first we extract the date column, then we cbind it with the stocks closing prices
#PSTD : popular stocks for today

PSTD.Date<-Stock[[toString(TPS[1])]][,1]
#PSTD.Date
PSTD<-as.data.frame(cbind(PSTD.Date))
#PSTD
#Stock[[toString(TPS[1])]][,c(1,5)]

#prepare the data set for ploting, include all popular stocks for today into one dataframe: PSTD
#using dplyr here
library(dplyr)
for (i in 1:n){
  PSTD<-full_join(PSTD, Stock[[toString(TPS[i])]][,c(1,5)], by="date")
  #ncol(PSTD)
  #PSTD<-cbind(PSTD, Stock[[toString(TopPS_list[i])]][,5]), abandoned because using merge ensures if different stock price has different history length
  colnames(PSTD)[ncol(PSTD)]<-toString(TPS[i])
  #summary(PSTD)
}
PSTD

#start ploting
library(ggplot2)
#?ggplot2

#melt the dataframe first to plot multiple lines
library(reshape2)
PSTD.long<-melt(PSTD,id.vars="date")
#head(PSTD.long)
colnames(PSTD.long)[2]<-"Popular_Stock"
#summary(PSTD.long)

#string manipulate the plot title
plot_title<-gsub("n", n, "Top n Stocks")
#plot_title

#generate the final plot
help.search("geom_", package = "ggplot2")

gg<-ggplot(PSTD.long, 
           aes(x=date, y=value, color=Popular_Stock)) + geom_line() + geom_point() + labs(title=plot_title)

print(gg)

#####

#here's the lower parenthesis of user defined function
}

RoyStocks(4)
